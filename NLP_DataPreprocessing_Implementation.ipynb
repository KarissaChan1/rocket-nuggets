{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKCOttsJ8QKGQLkVjkZTTH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarissaChan1/rocket-nuggets/blob/main/NLP_DataPreprocessing_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A summary tutorial of data preprocessing methods for NLP applications, including LSTM architecture for sentiment analysis."
      ],
      "metadata": {
        "id": "mRDM4mNKZDlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "lOedQIzk9p7P"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "ISssofne9tNY"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "BxtO3m_NZCZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_csv = '/content/IMDB Dataset.csv'\n",
        "df = pd.read_csv(base_csv)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "snh-ZxBB9tk0",
        "outputId": "17737c0e-976d-403c-801b-42fc62ace0c8"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5f12e87-30a1-4ec6-b52c-375ac025331c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5f12e87-30a1-4ec6-b52c-375ac025331c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5f12e87-30a1-4ec6-b52c-375ac025331c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5f12e87-30a1-4ec6-b52c-375ac025331c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train/test sets and encode positive/negative labels"
      ],
      "metadata": {
        "id": "owAgd5IHZUs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = df['review'].values,df['sentiment'].values\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')\n",
        "\n",
        "\n",
        "encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
        "encoded_test = [1 if label =='positive' else 0 for label in y_test] \n",
        "print(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtkaOZBg-oWH",
        "outputId": "2a66d467-53e1-49c1-e49d-540d7bee3950"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (37500,)\n",
            "shape of test data is (12500,)\n",
            "['I am not a fan of Sean Penn, but in contrast to my German colleague whose review appears here, I think he was perfectly cast as the neurotic, druggy character in this film. He has every nuance perfected and reminded me of several acquaintances who had similar tastes in \"recreational chemistry.\" I saw this film but once, 10-15 years ago and this is the only part of the film that was etched indelibly on my mind. I don\\'t say it very often, but in this case I will: Bravo, Sean Penn! As for the story line, well, it\\'s based on fact, and as such, it is a tragedy that people would sell their country\\'s secrets to the then enemy. Again, Penn has shown what you can do if you disagree with the administration. Use the freedoms you have, paid for in blood; don\\'t break the law.'\n",
            " '\"Chairman of the Board\" is a ridiculously stupid film from the popular comic Carrot Top (also seen on the 1-800-COLLECT commercials). He plays a surfing inventor who comes upon a man who has a flat tire. Top helps him out and a few days later discovers the guy\\'s died and has given his company to the comedian. Even if someone else was in Carrot Top\\'s role, it still would have been bad. The jokes (which are constantly rigged throughout) are terrible and the idea of a romantic \"plot twist\" should have been discarded. DO NOT WATCH!!!'\n",
            " 'I have done quite a few reviews on IMDb and this film is unique in that I never saw the entire movie. It was so terribly stupid and unfunny that I just couldn\\'t sit through it--though I tried.<br /><br />The biggest problem with this and any Mel Brooks movie I call \"the Mel quotient\". In other words, the better his movie, the less of him you see in the film. Think about it--The Producers and Young Frankenstein were great films and he was barely in them at all. BUT, films like Life Stinks and Space Balls are chock full of Mel and are pretty dopey movies (yes, I DID NOT LIKE Space Balls--but this isn\\'t the place to talk about that).<br /><br />Second, apart from cancer, rectal itch and mental retardation, I can\\'t think of a less funny topic than homelessness. This is just a comedy breaker. Think about it, folks. The FUNNIEST(?) scene in the movie has Mel making whoopee with Leslie Ann Warren in a dumpster!! And this is funny in what way?'\n",
            " ...\n",
            " 'This movie is one of the worst movies I have ever seen. There is absolutely no storyline, the gags are only for retards and there is absolutely nothing else that would make this movie worth watching. In the whole movie Fredi (oh my god what a funny name. ha ha) doesn\\'t ask himself ONCE how he came from a plane to middle earth. There are plenty of stupid and totally unfunny characters whose names should sound funny. e.g. : Gandalf is called Almghandi, Sam is called Pupsi ... and so on. I didn\\'t even smile once during the whole movie. The gags seem like they were made by people whose IQ is negative. If you laugh when someone\\'s coat is trapped in the door (this happens about 5 times) then this movie is perhaps for you. Another funny scene: They try to guess the code word for a closed door (don\\'t ask why- don\\'t ever ask \"why\" in this movie) and the code word is (ha ha): dung. So if you laughed at this examples you might like this movie. For everybody else: Go to Youtube and watch \"Lord of the Weed\": it\\'s a lot, lot more fun.'\n",
            " 'This early Warner Brothers talkie \"Son of the Gods\" (1930) deals with the racial intolerance that Anglo-Saxon Americans show towards the Chinese. Chinese-Americans are treated like second-class citizens, and whites hold them in nothing but contempt.<br /><br />Prolific scenarist Bradley King based her screenplay on Rex Beach\\'s novel about a young, impressionable Chinaman, Sam Lee (Richard Barthelmess of \"Only Angels Have Wings\"), who experiences racial prejudice first-hand when the girls that his college chums bring along for a party reveal their racist sentiments about Sam once they learn about his heritage. Sam goes to his father, Lee Ying (E. Alyn Warren of \"Gone With The Wind\"), who is a wealthy Chinaman with offices not only in New York City but also in San Francisco. Sam feels deeply wounded by the racial slurs and he wants to leave New York and go where he cannot be hurt by Americans. His patient father warns him that racism is a fact of everyday life and the only solution to racism is tolerance. Sam has yet to learn this lesson. He refuses to take any more money from his father and catches a ship to London, England, peeling potatoes while he is on board.<br /><br />During the trip, he encounters a British playwright, Bathurst (Claude King of \"Arrowsmith\"), who needs some help writing a play about the Chinese. Sam and he strike up a friendship and Sam furnishes him with cultural information about Asians. While they are relaxing in France, Sam meets a beautiful young woman, Allana Wagner (Constance Bennett of \"Two-Faced Woman\"), who falls madly in love with him. It seems that Allana and her wealthy father are vacationing in the same motel. Everybody at the motel knows about Sam being a Chinaman with the exception of Allana. Sensitive about his racial heritage, Sam holds Allana at arm\\'s length until she convinces him that nothing could change her mind about him. They fall madly in love together. Allana\\'s father drops the bomb on her when he reveals that Sam is a Chinaman and all the memories of living in San Francisco and dealing with coolies floods Allana\\'s mind. She storms into the dining room at the motel and publicly flogs Sam with a riding crop in front of a room filled with on-lookers.<br /><br />Of course, Sam is terribly devastated by this reversal of events. He thought that Allana loved him but she didn\\'t. About this time, Sam\\'s father Lee Ying falls tragically ill and Ying\\'s secretary of sorts, Eileen (Mildred Van Dorn of \"Iron Man\") sends Sam a telegram about Ying\\'s illness. Predictably, Sam rushes home to New York to be at his father\\'s side. Since his public humiliation, Sam has vowed to show no kindness to Anglo-Saxon Americans; Eileen is an Irish-Catholic and probably one of his few white friends. Lee Ying dies and Sam assumes control of the business and he practices his anti-White racism, until he learns that he was an Anglo-Saxon foundling that a San Francisco cop on the beat gave to Lee Ying and his wife to bring up. The cop forgot about it until two white busy-bodied social worker types wanted to take Sam away from the Yings. Sam learns this revelation about the same time that Allana comes to New York and falls ill. During her illness, she utters his name repeatedly in her sleep and her devoted father goes to see Sam and requests that Sam visit her in order to help her recover. Unbeknownst to Allana, Sam does visit her and she improves, but she has no memory of his visit, merely a hazy notion. Eventually, Allana learns the truth about Sam not being a Chinaman and they marry and live happily ever after.<br /><br />This socially conscientious Warner Brothers/First National Pictures Release contends frankly and unflinchingly with the race issue for the first hour or thereabouts before the revelation that Sam has no Chinese blood running in his veins catches both him as well as the audience by surprise. The reconciliation between Allana and Sam stretches credibility, despite their self-professed undying love for each other. However, in the name of a happy ending that would erase all the negativity that came before it, they wind up in each other\\'s arms.<br /><br />The capitulation on the race issue with the revelation that Sam isn\\'t Chinese damages some of the film\\'s moral power. Incredibly, \"Son of the Gods\" is a Pre-Code film that almost seems prudish; for example, Sam is an American, not Chinese! Constance Bennett gives a wonderful performance as a petulant beautify and she holds your attention when she whips Sam with her riding crop. Claude King is good as Bathurst, and E. Alyn Warren is convincing as Lee Ying. Interestingly, Warren made a career out of portraying Asian characters. Richard Barthelmess is flawless as Sam; he delivers a highly nuanced performance. Despite its age, \"Son of the Gods\" is a son of a good movie!'\n",
            " 'J.S. Cardone directed a little known \\'Video Nasty\\' in 1982 called \"The Slayer\" and since then has gone on to have a hand in a handful of feature films; including the rubbish 2001 vampire movie The Forsaken. His latest feature film, Wicked Little Things, boasts a plot that sounds decent as well as a creepy looking poster that I seem to remember surfacing a couple of years ago in relation to a film that Tobe Hooper was meant to direct. Well I guess he felt that this one was too similar to his silly zombie fungus movie \\'Mortuary\\' and so turned this one down. I don\\'t blame him for it either. The plot focuses on a mother and her two daughters that move to an old house in the mountains that once belonged to her late husband. However, what they don\\'t realise is that around a hundred years earlier; a group of children that were being used as miners were trapped down a mineshaft. Naturally, that\\'s not the end of them and they managed to survive their ordeal and now prowl the area in search of revenge\\x85 <br /><br />The film is essentially a collection of clich√©s; from the youngest kid with an \"imaginary friend\", the mother who dismisses it and all the usual zombie rubbish. J.S. Cardone attempts to get the horror fans back on side with shocks and gory scenes (mostly involving kids) but its not enough. The story doesn\\'t play out very well at all either and really did remind me too much of the earlier Mortuary, and that\\'s not a good thing (although Mortuary is actually a better film than this one). The acting is nothing to write home about either; Lori Heuring is decent looking, as is eldest daughter Scout Taylor-Compton; but neither manages to provide an interesting performance. Chloe Moretz is slightly better than the usual child actor. The plot is given hardly any credibility and indeed the screenplay can\\'t even be bothered to explain the reasons why the kids attack the locals. It all boils down to a typical and rather dull ending and overall I have to say that if you know your horror movies, then you can feel free to skip this one!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize text"
      ],
      "metadata": {
        "id": "oEs2zz4fY_2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Function for text normalization\n",
        "def normalize_text(text_sample):\n",
        "  \n",
        "  tokenized = []\n",
        "  for i in range(len(text_sample)):\n",
        "    text = text_sample[i]\n",
        "    # Convert to lowercase\n",
        "    normalized_text = text.lower()\n",
        "\n",
        "    # Remove all non-word characters (everything except numbers and letters)\n",
        "    normalized_text = re.sub(r\"[^\\w\\s]\", '', normalized_text)\n",
        "  \n",
        "    # Tokenize\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "    tokens = tokenizer(normalized_text)\n",
        "\n",
        "    # Perform stemming\n",
        "    # stemmer = PorterStemmer()\n",
        "    # stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "    \n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    tokenized.append(words)\n",
        "  return tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE1edrOi_YUY",
        "outputId": "bcf6e2e4-64c5-40b6-d7e3-7e781f1587dc"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_normalized = normalize_text(x_train)\n",
        "x_test_normalized = normalize_text(x_test)"
      ],
      "metadata": {
        "id": "gQM6TGw_AU4Q"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_normalized[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsTVulIID7sp",
        "outputId": "e7a29a1b-cbae-4545-f706-9e4b5e48e656"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fan', 'sean', 'penn', 'contrast', 'german', 'colleague', 'whose', 'review', 'appears', 'think', 'perfectly', 'cast', 'neurotic', 'druggy', 'character', 'film', 'every', 'nuance', 'perfected', 'reminded', 'several', 'acquaintances', 'similar', 'tastes', 'recreational', 'chemistry', 'saw', 'film', '1015', 'years', 'ago', 'part', 'film', 'etched', 'indelibly', 'mind', 'dont', 'say', 'often', 'case', 'bravo', 'sean', 'penn', 'story', 'line', 'well', 'based', 'fact', 'tragedy', 'people', 'would', 'sell', 'countrys', 'secrets', 'enemy', 'penn', 'shown', 'disagree', 'administration', 'use', 'freedoms', 'paid', 'blood', 'dont', 'break', 'law']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform padding based on maximum sequence length"
      ],
      "metadata": {
        "id": "IOqNTaU3ZdGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_max_list(list):\n",
        "    list_len = [len(i) for i in list]\n",
        "    print(max(list_len))\n",
        "    return max(list_len)\n",
        "\n",
        "def padding_(sentences, seq_len):\n",
        "  for i,sent in enumerate(sentences):\n",
        "    if len(sent)<seq_len:\n",
        "      pad = seq_len - len(sent)\n",
        "      sentences[i] = (sentences[i] + [0] * pad)[:pad]\n",
        "\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "HOL9Aak8FZg8"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_train = find_max_list(x_train_normalized)\n",
        "max_len_test = find_max_list(x_test_normalized)\n",
        "\n",
        "padded_xtrain = np.asarray(padding_(x_train_normalized,max_len_train))\n",
        "padded_xtest = np.asarray(padding_(x_test_normalized,max_len_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbUMAt6wFaMH",
        "outputId": "496e2f07-60ae-44b2-f41b-9659a5ad2693"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1191\n",
            "1449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-137-2b806d4a4a94>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  padded_xtrain = np.asarray(padding_(x_train_normalized,max_len_train))\n",
            "<ipython-input-137-2b806d4a4a94>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  padded_xtest = np.asarray(padding_(x_test_normalized,max_len_train))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You may want to add a DataLoader code section here to batch the training data before giving it to word2vec\n",
        "# 1. create tensor dataset (combined xtrain and ytrain, xtest and ytest), print to make sure the tensor encodings are correct\n",
        "# 2. create DataLoaders\n",
        "# 3. train_dataloader is the new word2vec input"
      ],
      "metadata": {
        "id": "bzLAgPhwLT6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train word embedding model (word2vec) to use as embedding layer in LSTM architecture"
      ],
      "metadata": {
        "id": "Glhg-lBeZiNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize and train the Word2Vec model\n",
        "model = Word2Vec(padded_xtrain, vector_size=50, window=5, min_count=1, workers=1)\n",
        "\n",
        "# Retrieve word embeddings\n",
        "word_vector = model.wv['student']\n",
        "\n",
        "# Find most similar words\n",
        "similar_words = model.wv.most_similar('student')\n",
        "\n",
        "print(\"Word Embedding for 'student':\", word_vector)\n",
        "print(\"Most similar words to 'student':\", similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxIb9DIjHOYZ",
        "outputId": "b2f266df-553d-4e62-fbc7-1225e95640dd"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embedding for 'student': [-1.3708901  -0.43507367  0.18664095  0.6343911  -1.1630728   2.304918\n",
            " -0.29693615  0.24211895 -0.30435854  0.1929767  -0.47227374  0.14566927\n",
            "  0.7573182   0.6662409  -0.51274943 -0.02596606  1.614102    0.79449666\n",
            "  1.8662318   0.9794432   1.054361   -1.1728565   2.0218368  -0.7832905\n",
            "  1.7774361   0.22244166 -0.63765186 -0.8793735  -0.6945737   1.3244457\n",
            " -1.8467313   0.3278806  -0.33175245  0.28065422 -0.39990792 -0.08560283\n",
            "  0.3618493   0.07644077  0.3875066  -1.1224761   0.6221601   0.98940927\n",
            " -0.39729217 -0.7359131   0.45047677  1.6809639   0.6723763   0.05493571\n",
            "  1.681981   -0.82995003]\n",
            "Most similar words to 'student': [('teacher', 0.7724525928497314), ('students', 0.7599766850471497), ('class', 0.753632664680481), ('freshman', 0.7532499432563782), ('classes', 0.7451688051223755), ('graduate', 0.7425984144210815), ('college', 0.7266989946365356), ('boarding', 0.7167869210243225), ('studying', 0.7118719816207886), ('indian', 0.6949463486671448)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save weights of model to be used in pretrained nn.Embedding layer"
      ],
      "metadata": {
        "id": "kANX7G2EZqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.wv"
      ],
      "metadata": {
        "id": "HRSs9A6VJ9wC"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM architecture\n",
        "\n",
        "Embedding layer uses pre-trained weights from above. If not using word2vec or any other pretrained embedding model, use nn.Embedding(*args) layer which will be trained with the entire LSTM model."
      ],
      "metadata": {
        "id": "M1wzE_GQZxhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,weights,no_layers,vocab_size,hidden_dim,embedding_dim,output_dim,drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        " \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        " \n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.weights = weights\n",
        "    \n",
        "        # using word2vec as embedding layer: take model weights and set freeze = True\n",
        "        # or else self.embedding = nn.Embedding(*args) to train embedding layer while training entire model\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.weights,freeze=True) \n",
        "        \n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "        \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "X8xci-6vId0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}